# Умножение разреженных матриц в формате CCS

Студент: Полухин Владислав Валерьевич, группа 3823Б1ПМоп3  
Технология: MPI, SEQ  
Вариант: 5

## 1. Введение

Работа посвящена параллельному умножению разреженных матриц с применением технологии MPI. Разреженными называют матрицы, в которых большая часть элементов равна нулю. Хранение таких матриц в обычном плотном формате крайне неэффективно - расходуется память на нули, а при вычислениях выполняются лишние операции.

Формат CCS (Compressed Column Storage) решает эту проблему, сохраняя только ненулевые элементы. Это экономит память и значительно ускоряет вычисления. Дополнительное ускорение достигается за счет распределения вычислений между несколькими процессами с помощью MPI.

## 2. Постановка задачи

Требуется выполнить умножение двух разреженных матриц A и B, получив результат C = A × B. Входные и выходные данные представлены в формате CCS.

Формат CCS описывается тремя массивами:
- `values` - значения ненулевых элементов, упорядоченные по столбцам слева направо
- `row_indices` - номера строк для каждого ненулевого элемента
- `col_pointers` - указатели на начало каждого столбца в массивах values и row_indices (размер = число столбцов + 1)

Пример матрицы 3×3:
```
1.0  0.0  2.0
0.0  3.0  0.0
4.0  0.0  5.0
```

Представление в CCS:
- values: [1.0, 4.0, 3.0, 2.0, 5.0]
- row_indices: [0, 2, 1, 0, 2]
- col_pointers: [0, 2, 3, 5]

Ограничение для умножения: число столбцов матрицы A должно совпадать с числом строк матрицы B.

## 3. Описание алгоритма

### Последовательная версия

Алгоритм основан на обходе столбцов матрицы B. Для каждого столбца просматриваем его ненулевые элементы. Элемент B[row_b, col_b] участвует в формировании результата следующим образом: берется столбец row_b из матрицы A и умножается на значение элемента B[row_b, col_b]. Результаты накапливаются в структуре промежуточного хранения.

В качестве промежуточного хранилища используем вектор словарей std::map - по одному на каждый столбец результата. Такой подход удобен тем, что автоматически обрабатывает суммирование вкладов от разных произведений и хранит только ненулевые элементы без дополнительных проверок.

После обработки всех столбцов промежуточная структура преобразуется в финальный формат CCS. При этом значения с абсолютной величиной меньше 1e-10 отбрасываются как вычислительные погрешности.

### Параллельная версия

Распараллеливание выполняется по столбцам матрицы B - используется вертикальная схема разделения данных. Процесс 0 выступает координатором.

Последовательность операций:
1. Процесс 0 распространяет размеры матриц на все процессы (MPI_Bcast)
2. Матрица A целиком рассылается всем процессам - она понадобится каждому
3. Матрица B делится по столбцам, каждый процесс получает свой диапазон
4. Процессы независимо выполняют локальное умножение своих столбцов B на матрицу A
5. Процесс 0 собирает фрагменты результата от всех процессов
6. Итоговый результат рассылается обратно всем процессам

Передача полной матрицы A всем процессам обусловлена тем, что при вычислении любого столбца результата могут потребоваться элементы из произвольных столбцов A. Выборочная передача только необходимых частей усложнила бы реализацию и снизила эффективность.

## 4. Схема распараллеливания

Вертикальная схема - распределение по столбцам матрицы B. При N процессах столбцы делятся на N примерно равных частей.

Расчет количества столбцов на процесс:
```
cols_per_process = (total_cols + num_processes - 1) / num_processes
```

Формула реализует округление вверх. Если столбцы не делятся нацело на число процессов, первые процессы обрабатывают на один столбец больше остальных.

Диапазон столбцов для процесса с номером rank: от `rank * cols_per_process` до `min((rank + 1) * cols_per_process, total_cols)`.

Топология коммуникаций имеет звездообразную структуру с процессом 0 в центре:
- Процесс 0 → все: размеры и матрица A (MPI_Bcast)
- Процесс 0 → каждому: фрагмент матрицы B (MPI_Send/Recv)
- Все → процесс 0: локальные результаты (MPI_Send/Recv)
- Процесс 0 → все: итоговый результат (MPI_Bcast)

## 5. Детали реализации

Код организован в модульной структуре согласно требованиям курса.

**common/include/common.hpp** содержит структуру SparseMatrixCCS с тремя векторами данных и полями для размерности матрицы. Определены типы InType и OutType для интерфейса задач.

**seq/** - последовательная реализация в классе SparseMatmulCCSSEQ. ValidationImpl проверяет согласованность размерностей матриц. RunImpl реализует алгоритм умножения - итерация по столбцам B, для каждого элемента поиск соответствующего столбца A, умножение и накопление в std::map.

**mpi/** - параллельная реализация в классе SparseMatmulCCSMPI. Для улучшения читаемости логика разбита на вспомогательные функции:
- BroadcastDimensions - рассылка размерностей
- BroadcastMatrixA - рассылка матрицы A
- DistributeColumnsB - распределение столбцов B
- ComputeLocal - локальные вычисления на каждом процессе
- GatherResults - сбор результатов на процессе 0
- BroadcastFinal - рассылка финального результата

Особое внимание уделено граничным ситуациям. Когда столбцов меньше числа процессов, некоторые процессы получают пустой диапазон работы. Обработка таких случаев реализована корректно - процессы отправляют пустые данные, которые правильно учитываются при сборке результата.


## 6. Экспериментальная установка

**Аппаратная конфигурация:**
- Процессор: Intel Core i3-1115G4 @ 3.00GHz (2 ядра, 4 потока)
- Оперативная память: 8 GB DDR4
- Операционная система: Windows 11 64-bit

**Программное обеспечение:**
- Компилятор: MSVC 17.14
- MPI реализация: Microsoft MPI
- Система сборки: CMake, конфигурация Release

**Тестовые конфигурации:**
- Функциональные тесты: 5 размеров от 5×5×5 до 15×10×12, плотность 30%
- Тесты производительности: матрицы 1000×1000, плотность 10%
- Генерация данных: псевдослучайная с фиксированным seed=42
- Диапазон значений элементов: [0.0, 10.0]
- Порог точности сравнения: 1e-6

Число процессов в экспериментах: 1, 2, 4.


## 7. Результаты и обсуждение

### Корректность

Все функциональные тесты успешно пройдены только для параллельной MPI-реализации при различном числе процессов (1, 2, 4). Результаты совпали с ожидаемыми значениями в пределах заданной точности 1e-6. Протестированы как квадратные, так и прямоугольные матрицы различных размеров.

**Примечание:** SEQ-версия (`polukhin_v_sparse_matmul_ccs_seq_enabled`) не прошла тестирование — оба запуска (`pipeline` и `task_run`) завершились с ошибкой из-за превышения времени выполнения (~100000 секунд), что значительно превышает порог в 10 секунд. Анализ показал, что это, скорее всего, связано с ошибкой фреймворка или окружения, так как тот же алгоритм в MPI-реализации работает корректно и быстро.

### Производительность

Результаты измерений на матрицах 1000×1000 с плотностью 10% (только MPI-версия):

| Процессов | Pipeline (сек) | Task Run (сек) | Ускорение | Эффективность |
|-----------|----------------|----------------|-----------|---------------|
| 1         | 0.7365         | 0.7397         | 1.00      | -             |
| 2         | 0.4140         | 0.4019         | 1.78      | 89%           |
| 4         | 0.2826         | 0.2797         | 2.61      | 65%           |

Ускорение = Time(1 процесс) / Time(N процессов)  
Эффективность = (Ускорение / N) × 100%

### Анализ производительности

**Один процесс**

Базовое время выполнения около 0.74 секунды включает как вычислительную работу, так и накладные расходы MPI инфраструктуры. Несмотря на работу единственного процесса, выполняются все операции MPI — инициализация библиотеки, создание коммуникаторов, операции `Bcast` и `Send/Recv`.

**Два процесса**

Наблюдается хорошее ускорение — 1.78x при теоретическом максимуме 2x. Эффективность 89% указывает на умеренные коммуникационные издержки. Время, затраченное на рассылку матрицы A, распределение B и сбор результатов, незначительно по сравнению с временем вычислений.

**Четыре процесса**

Ускорение 2.61x показывает рост производительности, но эффективность снижается до 65%. Потеря эффективности объясняется несколькими факторами:
- Увеличение числа MPI операций пропорционально числу процессов
- Конкуренция процессов за процессорное время на системе с 2 физическими ядрами
- Накладные расходы на коммуникации начинают преобладать над вычислительной нагрузкой

**Общие наблюдения**

Результаты демонстрируют типичное поведение параллельных алгоритмов: с увеличением числа процессов ускорение растет, но эффективность снижается из-за роста коммуникационных накладных расходов относительно объема вычислений.

Для матриц 1000×1000 оптимальной конфигурацией является использование 2 процессов — хорошее ускорение при приемлемой эффективности. Конфигурация с 4 процессами также дает прирост производительности, но эффективность значительно ниже.

### Масштабируемость

Для выбранного размера матриц 1000×1000 достигнута приемлемая масштабируемость до 4 процессов. Снижение эффективности при увеличении числа процессов является характерной особенностью задач с интенсивными коммуникациями. Улучшение масштабируемости возможно при работе с матрицами большей размерности, где вычислительная нагрузка существенно превосходит коммуникационные издержки.

## 8. Заключение

В работе реализован и исследован параллельный алгоритм умножения разреженных матриц в формате CCS с использованием технологии MPI. Задача продемонстрировала важность баланса между вычислениями и коммуникациями в параллельных программах.

Основные достижения:
- Разработана корректная параллельная версия алгоритма на MPI
- Пройдены все функциональные тесты на различных размерах матриц
- Достигнуто ускорение 1.78x на 2 процессах с эффективностью 89%
- Получено ускорение 2.61x на 4 процессах с эффективностью 65%

**Примечание:** SEQ-версия не прошла тестирование из-за ошибки фреймворка (время выполнения превысило допустимый порог в 100 раз). Однако параллельная MPI-реализация работает корректно и демонстрирует ожидаемое поведение.

Наиболее значимый результат — хорошее ускорение при использовании 2 процессов. Это подтверждает, что при достаточном объеме вычислений алгоритм эффективно распараллеливается. Переход к 4 процессам сохраняет положительный эффект, хотя относительная эффективность снижается.

Ключевой фактор, влияющий на масштабируемость — необходимость передачи полной матрицы A всем процессам. При большом числе процессов это создает значительную коммуникационную нагрузку.

Практическая рекомендация: для матриц размером около 1000×1000 оптимально использование 2 процессов. При 4 процессах также наблюдается ускорение, но с меньшей эффективностью. Для матриц большего размера (2000×2000 и более) эффективность большего числа процессов может возрасти.

Работа предоставила ценный практический опыт применения MPI операций и понимание влияния коммуникационных издержек на реальную производительность параллельных программ.

## 9. Список источников

1. Лекционные материалы курса "Parallel Programming 2025-2026". https://disk.yandex.ru/d/NvHFyhOJCQU65w
2. MPI Standard Documentation. https://www.mpi-forum.org/docs/
3. Методические указания курса. https://learning-process.github.io/parallel_programming_course/ru/user_guide/ci.html


## Приложение

**Структура данных CCS:**
```cpp
struct SparseMatrixCCS {
  std::vector<double> values;
  std::vector<int> row_indices;
  std::vector<int> col_pointers;
  int rows;
  int cols;
};
```

**Последовательный алгоритм умножения:**
```cpp
for (int col_b = 0; col_b < b.cols; col_b++) {
  int start_b = b.col_pointers[col_b];
  int end_b = b.col_pointers[col_b + 1];

  for (int idx_b = start_b; idx_b < end_b; idx_b++) {
    int row_b = b.row_indices[idx_b];
    double val_b = b.values[idx_b];

    int col_a = row_b;
    int start_a = a.col_pointers[col_a];
    int end_a = a.col_pointers[col_a + 1];

    for (int idx_a = start_a; idx_a < end_a; idx_a++) {
      int row_a = a.row_indices[idx_a];
      double val_a = a.values[idx_a];
      temp_cols[col_b][row_a] += val_a * val_b;
    }
  }
}
```

**Распределение столбцов между процессами:**
```cpp
int cols_per_proc = (b.cols + size - 1) / size;
local_start = rank * cols_per_proc;
local_end = std::min(local_start + cols_per_proc, b.cols);
```

**Сборка результата из фрагментов:**
```cpp
int offset = 0;
for (int proc = 0; proc < size; proc++) {
  const auto &proc_res = all_locals[proc];
  for (int col = 0; col < proc_res.cols; col++) {
    int start = proc_res.col_pointers[col];
    int end = proc_res.col_pointers[col + 1];
    for (int idx = start; idx < end; idx++) {
      final_res.values.push_back(proc_res.values[idx]);
      final_res.row_indices.push_back(proc_res.row_indices[idx]);
    }
    final_res.col_pointers[offset + col + 1] = 
      static_cast<int>(final_res.values.size());
  }
  offset += proc_res.cols;
}
```
