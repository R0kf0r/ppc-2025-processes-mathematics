# Умножение разреженных матриц в формате CCS

Студент: Полухин Владислав Валерьевич, группа 3823Б1ПМоп3  
Технология: MPI, SEQ  
Вариант: 5

## 1. Введение

В работе реализуется параллельное умножение разреженных матриц с помощью MPI. Разреженные матрицы - это матрицы, где большинство элементов равны нулю. Хранить их в обычном виде неэффективно, поэтому используются специальные форматы.

Формат CCS (Compressed Column Storage) хранит только ненулевые элементы, что экономит память и ускоряет вычисления. Параллелизация на MPI позволяет еще больше ускорить работу, распределив вычисления между несколькими процессами.

## 2. Постановка задачи

Нужно перемножить две разреженные матрицы A и B, получив результат C = A × B. Все матрицы представлены в формате CCS.

Формат CCS использует три массива:
- `values` - ненулевые элементы, упорядоченные по столбцам
- `row_indices` - номера строк для каждого элемента
- `col_pointers` - указатели на начало каждого столбца (размер = количество столбцов + 1)

Пример. Матрица 3×3:
```
1.0  0.0  2.0
0.0  3.0  0.0
4.0  0.0  5.0
```

В формате CCS:
- values: [1.0, 4.0, 3.0, 2.0, 5.0]
- row_indices: [0, 2, 1, 0, 2]
- col_pointers: [0, 2, 3, 5]

Для умножения A × B нужно, чтобы количество столбцов A равнялось количеству строк B.

## 3. Описание алгоритма

### Последовательная версия

Проходим по всем столбцам матрицы B. Для каждого столбца берем его ненулевые элементы. Каждый элемент B[row_b, col_b] умножаем на соответствующий столбец матрицы A (номер столбца в A равен row_b).

Результаты умножения накапливаем во временной структуре - используем std::map для каждого столбца результата. Это удобно, потому что позволяет автоматически суммировать вклады от разных умножений и хранить только ненулевые элементы.

После обработки всех столбцов преобразуем временную структуру в формат CCS. Элементы меньше 1e-10 отбрасываем как погрешности вычислений.

### Параллельная версия

Делим работу между процессами по столбцам матрицы B. Процесс 0 координирует всю работу.

Этапы:
1. Процесс 0 рассылает всем размеры матриц через MPI_Bcast
2. Процесс 0 рассылает всем полную матрицу A (она нужна каждому)
3. Матрица B делится на части по столбцам, каждый процесс получает свою часть
4. Каждый процесс независимо умножает свои столбцы B на матрицу A
5. Процесс 0 собирает результаты от всех процессов
6. Результат рассылается обратно всем процессам (для тестов)

Матрицу A передаем всем, потому что для любого столбца результата могут понадобиться элементы из любых столбцов A. Передавать только нужные части A сложно и неэффективно.

## 4. Схема распараллеливания

Распараллеливание идет по столбцам матрицы B. Если у нас N процессов, делим столбцы на N примерно равных частей.

Формула распределения:
```
cols_per_process = (total_cols + num_processes - 1) / num_processes
```

Это округление вверх - если столбцов не делится нацело, первые процессы получат на один столбец больше.

Каждый процесс обрабатывает столбцы от `rank * cols_per_process` до `min((rank + 1) * cols_per_process, total_cols)`.

Топология коммуникаций - "звезда" с процессом 0 в центре:
- Процесс 0 → все: размеры и матрица A (MPI_Bcast)
- Процесс 0 → каждому: часть матрицы B (MPI_Send/Recv)
- Все → процесс 0: части результата (MPI_Send/Recv)
- Процесс 0 → все: полный результат (MPI_Bcast)

## 5. Детали реализации

Код разбит на модули по требованиям курса.

**common/include/common.hpp** содержит структуру SparseMatrixCCS с тремя векторами для данных и двумя полями для размеров матрицы. Также определены типы InType и OutType для входных и выходных данных.

**seq/** - последовательная версия в классе SparseMatmulCCSSEQ. Метод ValidationImpl проверяет, что размеры согласованы. Метод RunImpl реализует умножение - проходит по столбцам B, для каждого элемента находит нужный столбец A, умножает и накапливает результат в std::map.

**mpi/** - параллельная версия в классе SparseMatmulCCSMPI. Логика разбита на вспомогательные функции для читаемости:
- BroadcastDimensions - рассылает размеры
- BroadcastMatrixA - рассылает матрицу A
- DistributeColumnsB - распределяет части B между процессами
- ComputeLocal - локальное умножение на каждом процессе
- GatherResults - сбор результатов на процессе 0
- BroadcastFinal - рассылка финального результата

Особое внимание - граничные случаи. Если столбцов меньше чем процессов, некоторые процессы ничего не делают (получают пустой диапазон). Это обрабатывается корректно - такие процессы отправляют пустые данные.

## 6. Экспериментальная установка

**Железо:**
- Процессор: Intel Core i3-1115G4 @ 3.00GHz (2 ядра, 4 потока)
- Память: 8 GB DDR4
- ОС: Windows 11 64-bit

**Софт:**
- Компилятор: MSVC 17.14
- MPI: Microsoft MPI
- Сборка: CMake, Release

**Тесты:**
- Функциональные: 5 размеров от 5×5×5 до 15×10×12, плотность 30%
- Производительность: матрицы 500×500, плотность 10%
- Генерация данных: random с фиксированным seed=42
- Диапазон значений: 0.0 до 10.0
- Точность проверки: 1e-6

Количество процессов: 1, 2, 4.

## 7. Результаты и обсуждение

### Корректность

Все 5 функциональных тестов пройдены для обеих версий при 1, 2 и 4 процессах. Результаты совпали с ожидаемыми с точностью 1e-6. Проверены квадратные и прямоугольные матрицы разных размеров.

### Производительность

Замеры на матрицах 500×500 с плотностью 10% (только MPI, так как у SEQ версии баг с измерением времени):

| Процессов | Pipeline (сек) | Task Run (сек) | Ускорение (pipeline) | Эффективность |
|-----------|----------------|----------------|----------------------|---------------|
| 1         | 0.764          | 0.721          | 1.00                 | -             |
| 2         | 0.396          | 0.389          | 1.93                 | 96%           |
| 4         | 0.267          | 0.266          | 2.86                 | 72%           |

Формула ускорения: Time(1 proc) / Time(N proc)  
Эффективность: (Ускорение / N процессов) × 100%

**Важно:** SEQ версия показывает 68000+ секунд вместо реальных значений - это известный баг фреймворка для последовательных тестов производительности. Реальное время выполнения составляет несколько секунд, что видно по wall-clock time. Функциональные тесты SEQ работают нормально.

### Анализ

**Один процесс** 

Базовое время выполнения около 0.72-0.76 секунд. Это включает все накладные расходы MPI инфраструктуры - инициализацию библиотеки, создание коммуникаторов и так далее. Несмотря на то что работает только один процесс, MPI операции все равно выполняются, просто без реальной передачи данных по сети.

**Два процесса**

Тут получается почти идеальное ускорение - 1.93x при теоретическом максимуме 2x. Эффективность 96% - очень хороший показатель! Это говорит о том, что накладные расходы на коммуникации (рассылка матрицы A, распределение B, сбор результатов) минимальны по сравнению с временем вычислений. Оба режима показывают похожие результаты, что логично.

**Четыре процесса**

Ускорение 2.86x выглядит отлично на первый взгляд, но эффективность падает до 72%. Это означает что четверть вычислительной мощности теряется на коммуникации и синхронизацию. Основные причины:
- Процесс 0 становится узким местом - он должен раздать данные четырём процессам и собрать от них результаты
- На моей системе всего 2 физических ядра (4 логических потока), поэтому процессы конкурируют за ресурсы
- Больше процессов = больше MPI операций = больше времени на коммуникации

Интересно что оба режима (task_run и pipeline) дают практически идентичные результаты. Это логично, потому что они выполняют одни и те же вычисления, просто по-разному организуют запуск.

**Общие наблюдения**

Результаты показывают типичное поведение параллельных алгоритмов с коммуникациями. До определённого момента (в нашем случае 2 процесса) ускорение почти линейное. Дальше начинается снижение эффективности из-за роста коммуникационных накладных расходов.

Для матриц 500×500 оптимальным получается использование 2 процессов. Переход на 4 процесса даёт прирост производительности, но эффективность падает заметно. На больших матрицах (1000×1000 и выше) картина может быть другой - вычислений становится намного больше относительно коммуникаций.

### Масштабируемость

Для матриц 500×500 получено приемлемое ускорение до 4 процессов. Эффективность снижается с ростом числа процессов - типичная картина для задач с коммуникациями. Для лучшей масштабируемости нужны матрицы побольше (1000×1000+), где вычислений намного больше относительно передачи данных.

## 8. Заключение

В работе реализован параллельный алгоритм умножения разреженных матриц в формате CCS с использованием MPI. Задача оказалась интересной с точки зрения балансировки вычислений и коммуникаций.

Основные результаты:
- Последовательная и параллельная версии работают корректно
- Все функциональные тесты пройдены
- На 2 процессах получено ускорение 1.93x с эффективностью 96%
- На 4 процессах ускорение 2.86x с эффективностью 72%

Самое интересное наблюдение - почти линейное ускорение на 2 процессах. Это говорит о том, что алгоритм хорошо распараллеливается при условии, что вычислений существенно больше чем коммуникаций. При переходе на 4 процесса коммуникационные накладные расходы становятся заметнее, но общий прирост производительности всё ещё положительный.

Главный фактор, ограничивающий масштабируемость - необходимость передавать полную матрицу A всем процессам. Это создаёт значительный объём коммуникаций, особенно при большом количестве процессов. Альтернативные подходы (распределённое хранение A, блочные схемы) могли бы улучшить ситуацию, но усложнили бы реализацию.

Практический вывод: для матриц размером 500×500 оптимально использовать 2 процесса. Четыре процесса тоже дают выигрыш, но не такой значительный. Для больших матриц (1000×1000+) картина может измениться в пользу большего числа процессов.

Работа дала хороший опыт практического применения MPI операций и понимание того, как коммуникационные накладные расходы влияют на реальную производительность параллельных программ.

## 9. Список источников

1. Лекции "Parallel Programming 2025-2026". https://disk.yandex.ru/d/NvHFyhOJCQU65w
2. MPI Standard. https://www.mpi-forum.org/docs/
3. Документация курса. https://learning-process.github.io/parallel_programming_course/ru/user_guide/ci.html

## Приложение

**Структура CCS:**
```cpp
struct SparseMatrixCCS {
  std::vector<double> values;
  std::vector<int> row_indices;
  std::vector<int> col_pointers;
  int rows;
  int cols;
};
```

**Последовательное умножение:**
```cpp
for (int col_b = 0; col_b < b.cols; col_b++) {
  int start_b = b.col_pointers[col_b];
  int end_b = b.col_pointers[col_b + 1];

  for (int idx_b = start_b; idx_b < end_b; idx_b++) {
    int row_b = b.row_indices[idx_b];
    double val_b = b.values[idx_b];

    int col_a = row_b;
    int start_a = a.col_pointers[col_a];
    int end_a = a.col_pointers[col_a + 1];

    for (int idx_a = start_a; idx_a < end_a; idx_a++) {
      int row_a = a.row_indices[idx_a];
      double val_a = a.values[idx_a];
      temp_cols[col_b][row_a] += val_a * val_b;
    }
  }
}
```

**Распределение столбцов:**
```cpp
int cols_per_proc = (b.cols + size - 1) / size;
local_start = rank * cols_per_proc;
local_end = std::min(local_start + cols_per_proc, b.cols);
```

**Сбор результатов:**
```cpp
int offset = 0;
for (int proc = 0; proc < size; proc++) {
  const auto &proc_res = all_locals[proc];
  for (int col = 0; col < proc_res.cols; col++) {
    int start = proc_res.col_pointers[col];
    int end = proc_res.col_pointers[col + 1];
    for (int idx = start; idx < end; idx++) {
      final_res.values.push_back(proc_res.values[idx]);
      final_res.row_indices.push_back(proc_res.row_indices[idx]);
    }
    final_res.col_pointers[offset + col + 1] = 
      static_cast<int>(final_res.values.size());
  }
  offset += proc_res.cols;
}
```
