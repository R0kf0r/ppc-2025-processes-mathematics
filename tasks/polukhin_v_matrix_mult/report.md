# Ленточная горизонтальная схема умножения матриц

- **Студент:** Полухин Владислав Валерьевич, группа 3823Б1ПМоп3
- **Технология:** MPI
- **Вариант:** Ленточная горизонтальная схема (разбиение матрицы A по строкам)

## 1. Введение

В этой работе изучается применение параллельных алгоритмов для операций с матрицами через технологию MPI. Основная задача - реализовать умножение матриц с использованием ленточной горизонтальной схемы распараллеливания.

Умножение матриц встречается повсюду: машинное обучение, компьютерная графика, научные расчёты. Когда матрицы большие, обычные последовательные алгоритмы работают слишком медленно. Поэтому нужно распараллеливать вычисления.

## 2. Постановка задачи

Задача - реализовать параллельный алгоритм умножения двух матриц C = A × B, где:
- Матрица A размером m × k
- Матрица B размером k × n
- Результат C размером m × n

Каждый элемент результата вычисляется так:

C[i][j] = Σ(p=0 to k-1) A[i][p] × B[p][j]

где i от 0 до m-1, j от 0 до n-1.

В ленточной горизонтальной схеме матрица A разбивается по строкам между процессами. Каждый процесс обрабатывает свои строки и считает соответствующую часть результата.

## 3. Описание алгоритма

### Последовательная версия

Обычный алгоритм с тремя вложенными циклами:

1. Цикл по строкам матрицы A (i от 0 до m-1)
2. Цикл по столбцам матрицы B (j от 0 до n-1)
3. Цикл для скалярного произведения (p от 0 до k-1)

Для каждого элемента результата считается сумма произведений соответствующих элементов строки A и столбца B. Сложность O(m × n × k).

### Параллельная версия

**Шаг 1. Подготовка данных**

Процесс 0 имеет исходные матрицы A и B. Размеры (m, k, n) рассылаются всем процессам через MPI_Bcast. Матрица B тоже рассылается всем, потому что она нужна каждому для вычислений.

**Шаг 2. Распределение строк A**

Строки матрицы A распределяются между процессами максимально равномерно:
- base_rows = m / size
- extra_rows = m % size
- Первые extra_rows процессов получают base_rows + 1 строк
- Остальные получают base_rows строк

Для каждого процесса считается количество строк my_rows и начальная позиция my_start.

**Шаг 3. Передача данных**

MPI_Scatterv распределяет части матрицы A между процессами. Процесс 0 подготавливает массивы sendcounts и displs, которые показывают сколько и откуда отправить.

**Шаг 4. Вычисления**

Каждый процесс умножает свои строки A на всю матрицу B. Получается его часть результата C.

**Шаг 5. Сбор результатов**

MPI_Gatherv собирает все части результата обратно на процесс 0. Это соответствует ленточной горизонтальной схеме, где только матрица A разбивается, а матрица B полностью доступна каждому процессу.

## 4. Схема распараллеливания

Ленточная горизонтальная схема делит работу по строкам матрицы A.

**Распределение нагрузки**

Формулы для вычисления количества строк:
- base_rows = m / num_processes
- extra_rows = m % num_processes

Процессы с рангом < extra_rows получают base_rows + 1, остальные base_rows. Разница в нагрузке максимум одна строка.

**Коммуникации**

Схема "звезда" с процессом 0 в центре:
- Broadcast размеров: процесс 0 → все
- Broadcast матрицы B: процесс 0 → все (матрица B нужна целиком каждому процессу)
- Scatterv строк A: процесс 0 → каждому
- Gatherv результатов: все → процесс 0

**Использование памяти**

Процесс 0 хранит полные матрицы A и B. Каждый рабочий процесс хранит:
- Свои строки A (my_rows × k)
- Всю матрицу B (k × n)
- Свои строки результата (my_rows × n)

## 5. Детали реализации

Проект организован модульно.

**Общие типы (common/include/common.hpp)**

Структуры для данных:
- `MatrixDimensions` - размеры матриц
- `MatrixInput` - входные матрицы и размеры
- Типы InType, OutType для задачи

**Последовательная версия (seq/)**

Класс `MatrixMultTaskSEQ` реализует 4 метода:
- `ValidationImpl` - проверка входных данных
- `PreProcessingImpl` - выделение памяти
- `RunImpl` - умножение матриц
- `PostProcessingImpl` - финализация

**Параллельная версия (mpi/)**

Класс `MatrixMultTaskMPI` с аналогичной структурой. В `RunImpl` реализована параллельная логика с MPI операциями. Особенность реализации: матрица B передается целиком каждому процессу через MPI_Bcast, что соответствует ленточной горизонтальной схеме.

**Обработка граничных случаев**

Код корректно работает когда:
- Процессов больше чем строк (некоторые ничего не делают)
- Матрицы малого размера
- Размеры не делятся нацело

**Хранение матриц**

Матрицы хранятся в одномерных массивах. Элемент [i][j] находится по индексу i*cols + j. Это даёт последовательный доступ к памяти.

## 6. Экспериментальная установка

**Аппаратура:**
- CPU: Intel Core i3-1115G4 @ 3.00GHz (2 ядра, 4 потока)
- RAM: 8 GB DDR4
- OS: Windows 11 64-bit

**ПО:**
- Компилятор: MSVC 17.14
- MPI: Microsoft MPI
- Сборка: CMake, Release режим

**Параметры тестов:**
- Функциональные: 9 размеров от 1×1×1 до 20×15×25
- Производительность: матрицы 700×700
- Генерация: random с seed=42
- Значения: [-10.0, 10.0]
- Точность: 1e-6 (функциональные), 1e-3 (производительность)

Количество процессов: 1, 2, 4.

## 7. Результаты и обсуждение

### 7.1 Проверка корректности

Все 9 функциональных тестов успешно пройдены для последовательной и параллельной версий. Результаты совпали с ожидаемыми для всех размеров матриц и всех количеств процессов.

Протестированы разные случаи:
- Минимальные матрицы 1×1
- Квадратные разных размеров
- Прямоугольные матрицы
- Большие размеры 20×15×25

### 7.2 Анализ производительности

Измерения на матрицах 700×700 (pipeline режим):

| Версия | Процессов | Время (сек) | Ускорение | Эффективность |
|--------|-----------|-------------|-----------|---------------|
| SEQ    | 1         | 0.326       | 1.00      | -             |
| MPI    | 1         | 0.327       | 1.00      | 100%          |
| MPI    | 2         | 0.208       | 1.57      | 79%           |
| MPI    | 4         | 0.196       | 1.66      | 42%           |

**Примечание:** Режим task_run для SEQ версии показывает некорректное измерение времени (баг фреймворка), поэтому в таблице только результаты pipeline.

### 7.3 Выводы

**1 процесс**

MPI на одном процессе работает примерно так же, как последовательная версия (0.327 против 0.326 сек). Накладные расходы MPI минимальны.

**2 процесса**

Ускорение 1.57x, эффективность 79%. Хороший результат - время на коммуникации ещё небольшое по сравнению с вычислениями.

**4 процесса**

Ускорение 1.66x, эффективность 42%. Заметное снижение эффективности из-за:
- Увеличения коммуникационных затрат
- Конкуренции за 2 физических ядра (4 потока на 2 ядрах)
- Последовательных участков (broadcast матрицы B, gather результата)

**Сравнение подходов**

Умножение матриц показывает неплохую эффективность при распараллеливании. Много вычислений на единицу данных - коммуникации оправданы. Хотя для совсем линейного ускорения нужны ещё большие матрицы (1000×1000+), где вычисления полностью доминируют.

### 7.4 Масштабируемость

Для матриц 700×700 получено приемлемое ускорение. Эффективность падает при увеличении числа процессов, что типично для ленточной схемы. Для линейной масштабируемости нужны матрицы побольше, где вычислений намного больше чем коммуникаций.

## 8. Заключение

Реализован параллельный алгоритм умножения матриц с ленточной горизонтальной схемой на MPI. Код работает корректно и показывает приемлемую производительность.

**Достижения:**
- Реализованы последовательная и параллельная версии
- Все тесты пройдены
- Ускорение до 1.66x на 4 процессах
- Модульная организация кода
- Реализация соответствует ленточной горизонтальной схеме: только матрица A разбивается, матрица B полностью доступна каждому процессу

Анализ показал что эффективность зависит от размера матриц и числа процессов. Ленточная схема хорошо работает когда вычислений намного больше чем коммуникаций.

**Возможные улучшения:**
- Блочная схема для лучшей локальности кэша
- Гибрид MPI+OpenMP
- Оптимизация доступа к памяти
- Использование BLAS библиотек

Работа помогла разобраться с MPI операциями Broadcast, Scatterv, Gatherv и получить практический опыт параллельного программирования.

## 9. Список источников

1. Лекции "Parallel Programming 2025-2026". https://disk.yandex.ru/d/NvHFyhOJCQU65w
2. MPI Standard Version 4.0. https://www.mpi-forum.org/docs/
3. Документация курса. https://learning-process.github.io/parallel_programming_course/ru/
4. Гергель В.П. Теория и практика параллельных вычислений.
5. Peter Pacheco. Parallel Programming with MPI.

## Приложение

### Фрагменты кода

**Последовательное умножение:**
```cpp
for (size_t i = 0; i < m; ++i) {
  for (size_t j = 0; j < n; ++j) {
    double sum = 0.0;
    for (size_t p = 0; p < k; ++p) {
      double a_elem = a[i * k + p];
      double b_elem = b[p * n + j];
      sum += a_elem * b_elem;
    }
    result[i * n + j] = sum;
  }
}
```

**Распределение строк:**
```cpp
int base_rows = m / size;
int extra_rows = m % size;

if (rank < extra_rows) {
  my_rows = base_rows + 1;
  my_start = rank * (base_rows + 1);
} else {
  my_rows = base_rows;
  my_start = extra_rows * (base_rows + 1) + (rank - extra_rows) * base_rows;
}
```

**Локальное умножение:**
```cpp
for (int i = 0; i < my_rows; ++i) {
  for (int j = 0; j < n; ++j) {
    double sum = 0.0;
    for (int p = 0; p < k; ++p) {
      double val_a = local_a[i * k + p];
      double val_b = b_matrix[p * n + j];
      sum += val_a * val_b;
    }
    local_result[i * n + j] = sum;
  }
}
```
